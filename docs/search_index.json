[["index.html", "STAT 253: Statistical Machine Learning Welcome!", " STAT 253: Statistical Machine Learning Welcome! Image source This is the class site for Statistical Machine Learning (STAT 253) at Macalester College for Spring 2022. The content here was made by Bryan Martin and draws upon our class textbook, the 2nd edition of An Introduction to Statistical Learning with Applications in R, and on materials prepared by Brianna Heggeseth, Alicia Johnson, and Leslie Myint. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["schedule.html", "Schedule", " Schedule The schedule below is a tentative outline of our plans for the semester. Before each class period, please watch the indicated videos and check on your understanding by actively reviewing the associated Learning Goals. The readings listed below are optional but serve as a nice complement to the videos and class activities. Readings refer to chapters/sections in the Introduction to Statistical Learning (ISLR) textbook (available online here). Week 1 Date Topic Videos/Readings Slides/Notes 1/20 1 Introductions Slides Week 2 Date Topic Videos/Readings Slides/Notes 1/25 2 Evaluating Regression Models Evaluating Regression Models R: Introduction to TidyModels ISLR: Chap 1 & Sections 2.1, 2.2 Slides 1/27 3 Overfitting Overfitting R: Preprocessing and Recipes ISLR: 5.1 Slides Board Notes Start Homework 1 due Fri, 2/4 at 9:00 AM CST "],["learning-goals.html", "Learning Goals General Skills Course Topics", " Learning Goals The goal of this course is for you to further develop general skills necessary for statistics and data science and gain a working understanding of set of machine learning algorithms. Specific course topics and general skills are listed below. Use these to guide your synthesis of course material for your portfolio and project throughout the entire semester General Skills Computational Thinking Be able to perform the following tasks: Decomposition: Break a task into smaller tasks to be able to explain the process to another person or computer Pattern Recognition: Recognize patterns in tasks by noticing similarities and common differences Abstraction: Represent an idea or process in general terms so that you can use it to solve other projects that are similar in nature Algorithmic Thinking: Develop a step-by-step strategy for solving a problem Ethical Data Thinking Identify ethical issues associated with applications of statistical machine learning in a variety of settings Assess and critique the actions of individuals and organizations as it relates to ethical use of data Data Communication In written and oral formats: Inform and justify data analysis and modeling process and the resulting conclusions with clear, organized, logical, and compelling details that adapt to the background, values, and motivations of the audience and context in which communication occurs. Collaborative Learning Understand and demonstrate characteristics of effective collaboration (team roles, interpersonal communication, self-reflection, awareness of social dynamics, advocating for yourself and others). Develop a common purpose and agreement on goals. Be able to contribute questions or concerns in a respectful way. Share and contribute to the group’s learning in an equitable manner. Course Topics Specific learning objectives for our course topics are listed below. Use these to guide your synthesis of video and reading material for specific topics. Introduction to Statistical Machine Learning Formulate research questions that align with regression, classification, or unsupervised learning tasks Evaluating Regression Models Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way Overfitting and cross-validation Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Accurately describe all steps of cross-validation to estimate the test/out-of-sample version of a model evaluation metric Explain what role CV has in a predictive modeling analysis and its connection to overfitting Explain the pros/cons of higher vs. lower k in k-fold CV in terms of sample size and computing time Subset selection Clearly describe the forward and backward stepwise selection algorithm and why they are examples of greedy algorithms Compare best subset and stepwise algorithms in terms of optimality of output and computational time LASSO (shrinkage/regularization) Explain how ordinary and penalized least squares are similar and different with regard to (1) the form of the objective function and (2) the goal of variable selection Explain how the lambda tuning parameter affects model performance and how this is related to overfitting KNN Regression and the Bias-Variance Tradeoff Clearly describe / implement by hand the KNN algorithm for making a regression prediction Explain how the number of neighbors relates to the bias-variance tradeoff Explain the difference between parametric and nonparametric methods Explain how the curse of dimensionality relates to the performance of KNN Modeling Nonlinearity: Polynomial Regression and Splines Explain the advantages of splines over global transformations and other types of piecewise polynomials Explain how splines are constructed by drawing connections to variable transformations and least squares Explain how the number of knots relates to the bias-variance tradeoff Local Regression and Generalized Additive Models Clearly describe the local regression algorithm for making a prediction Explain how bandwidth (span) relate to the bias-variance tradeoff Describe some different formulations for a GAM (how the arbitrary functions are represented) Explain how to make a prediction from a GAM Interpret the output from a GAM Logistic regression Use a logistic regression model to make hard (class) and soft (probability) predictions Interpret non-intercept coefficients from logistic regression models in the data context Evaluating classification models Calculate (by hand from confusion matrices) and contextually interpret overall accuracy, sensitivity, and specificity Construct and interpret plots of predicted probabilities across classes Explain how a ROC curve is constructed and the rationale behind AUC as an evaluation metric Appropriately use and interpret the no-information rate to evaluate accuracy metrics Decision trees Clearly describe the recursive binary splitting algorithm for tree building for both regression and classification Compute the weighted average Gini index to measure the quality of a classification tree split Compute the sum of squared residuals to measure the quality of a regression tree split Explain how recursive binary splitting is a greedy algorithm Explain how different tree parameters relate to the bias-variance tradeoff Bagging and random forests Explain the rationale for bagging Explain the rationale for selecting a random subset of predictors at each split (random forests) Explain how the size of the random subset of predictors at each split relates to the bias-variance tradeoff Explain the rationale for and implement out-of-bag error estimation for both regression and classification Explain the rationale behind the random forest variable importance measure and why it is biased towards quantitative predictors (in class) K-means clustering Clearly describe / implement by hand the k-means algorithm Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Hierarchical clustering Clearly describe / implement by hand the hierarchical clustering algorithm Compare and contrast k-means and hierarchical clustering in their outputs and algorithms Interpret cuts of the dendrogram for single and complete linkage Describe the rationale for how clustering algorithms work in terms of within-cluster variation Describe the tradeoff of more vs. less clusters in terms of interpretability Implement strategies for interpreting / contextualizing the clusters Principal Component Analysis Explain the goal of dimension reduction and how this can be useful in a supervised learning setting Interpret and use the information provided by principal component loadings and scores Interpret and use a scree plot to guide dimension reduction "],["r-and-rstudio-setup.html", "R and RStudio Setup Troubleshooting", " R and RStudio Setup Before the first day of class, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions. Highly recommended: Change the default file download location for your internet browser. Generally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. It is highly recommended that you change this option so that your browser asks you where to save each file before downloading it. This page has information on how to do this for the most common browsers. Required: Download R and RStudio FIRST: Download R here. You will see three links “Download R for …” Choose the link that corresponds to your computer. SECOND: Download RStudio here. Click the button under step 2 to install the version of RStudio recommended for your computer. Highly Recommended: Watch this video made by Professor Lisa Lendway that describes essential configuration options for RStudio. Required: Install the most up-to-date versions of the required R packages for this course. An R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Open RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter. install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;readr&quot;, &quot;rmarkdown&quot;, &quot;broom&quot;, &quot;tidymodels&quot;)) If you get a message that says “There are binary versions available the source versions are later”” type no and press Enter. You will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again. Enter the command library(ggplot2) and hit enter. If you see the message Error in library(ggplot2) : there is no package called ggplot2, then there was a problem installing this package. Jump down to the Troubleshooting section below. (Any other messages that appear are fine, and a lack of any messages is also fine.) Repeat the above step for the commands: library(dplyr) library(readr) library(rmarkdown) library(broom) library(tidymodels) Quit RStudio. You’re done setting up! Optional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio. Troubleshooting Problem: You are on a Mac and getting the following error: Error: package or namespace load failed for ‘ggplot2’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘rlang’ Here&#39;s how to fix it: - First install the suite of Command Line Tools for Mac using the instructions [here](http://osxdaily.com/2014/02/12/install-command-line-tools-mac-os-x/). - Next enter `install.packages(&quot;rlang&quot;)` in the Console. - Finally check that entering `library(ggplot2)` gives no errors. "],["r-resources.html", "R Resources Tidymodels resources Tidyverse resources Visualization resources General R resources Free online textbooks Example code", " R Resources Tidymodels resources Tidymodels reference Tidy Modeling with R Emil Hvitfeldt ISLR Labs with Tidymodels Lucy D’Agostino McGowan’s Intro to Tidymodels Tidyverse resources Lisa Lendway’s COMP/STAT 112 website (with code examples and videos) John’s Hopkins Tidyverse course text Visualization resources ggplot2 reference Colors in R General R resources RStudio cheat sheets Data import tutorial R Programming Wikibook Debugging in R Article Video Free online textbooks R for Data Science Exploratory Data Analysis with R Advanced R Example code Creating new variables case_when() from the dplyr package is a very versatile function for creating new variables based on existing variables. This can be useful for creating categorical or quantitative variables and for creating indices from multiple variables. # Turn quant_var into a Low/Med/High version data &lt;- data %&gt;% mutate(cat_var = case_when( quant_var &lt; 10 ~ &quot;Low&quot;, quant_var &gt;= 10 &amp; quant_var &lt;= 20 ~ &quot;Med&quot;, quant_var &gt; 20 ~ &quot;High&quot; ) ) # Turn cat_var (A, B, C categories) into another categorical variable # (collapse A and B into one category) data &lt;- data %&gt;% mutate(new_cat_var = case_when( cat_var %in% c(&quot;A&quot;, &quot;B&quot;) ~ &quot;A or B&quot; cat_var==&quot;C&quot; ~ &quot;C&quot; ) ) # Turn a categorical variable (x1) encoded as a numerical 0/1/2 variable into a different quantitative variable # Doing this for multiple variables allows you to create an index data &lt;- data %&gt;% mutate(x1_score = case_when( x1==0 ~ 10, x1==1 ~ 20, x1==2 ~ 50 ) ) # Add together multiple variables with mutate data &lt;- data %&gt;% mutate(index = x1_score + x2_score + x3_score) "],["r-cheatsheet.html", "R Cheatsheet Stat 155 Review Data Structures and Purrr Package Tidymodels Functions Visualizing Fit Models Real Data Example: Regression Real Data Example: Classification Real Data Example: Clustering Real Data Example: Dimension Reduction", " R Cheatsheet This cheatsheet is a work in progress. It will be updated and improved throughout the semester. Stat 155 Review Standard Linear Regression Models To fit a linear model, start with a formula to specify variable roles and which variable is the predictor variable (left hand side before ~). The lm function formats variables for you by creating indicator variables for factor/categorical variables. # predictor variables have pluses between them formula &lt;- y ~ x1 + x2 # specifies least squares estimation of the linear model specified by the formula using the data provided fit_model &lt;- lm(formula, data) # provides an overall summary of the fit model fit_model %&gt;% summary() # provides a data frame of the estimated coefficients, standard errors, and information for individual coefficient hypothesis tests fit_model %&gt;% tidy() # provides many model summaries including rsquared and sigma (the estimated sd of residuals) fit_model %&gt;% glance() # provides a data set that includes the original variables as well as the residuals and fitted values; used to plot residuals fit_model %&gt;% augment() # predict outcome values based on the fit model for newdata fit_model %&gt;% predict(newdata) Standard Logistic Regression Models To fit a linear model, start with a formula to specify variable roles and which variable is the predictor variable (left hand side before ~). The glm function formats variables for you by creating indicator variables for factor/categorical variables. Make sure that your outcome variable y is an indicator variable (0/1 or FALSE/TRUE). # predictor variables have pluses between them formula &lt;- y ~ x1 + x2 # specifies maximum likelihood estimation of the generalized linear model specified by the formula using the data provided; when family = &#39;binomial&#39;, it fits a logistic regression model rather than a linear model fit_model &lt;- glm(formula, data, family = &#39;binomial&#39;) # provides an overall summary of the fit model fit_model %&gt;% summary() # provides a data frame of the estimated coefficients, standard errors, and information for individual coefficient hypothesis tests fit_model %&gt;% tidy() # predict probability of outcome values based on the fit model for newdata fit_model %&gt;% predict(newdata, type = &#39;response&#39;) # calculate specificity, false positive rate, false negative rate, and sensitivity threshold &lt;- 0.25 # you choose threshold augment(fit_model, type.predict =&#39;response&#39;) %&gt;% mutate(predOutcome = .fitted &gt;= threshold) %&gt;% count(y, predOutcome) %&gt;% mutate(correct = (y == predictOutcome)) %&gt;% group_by(y) %&gt;% # condition on the true y outcome value mutate(prop = n/sum(n)) Data Structures and Purrr Package Vectors and Lists In R, there are many data structures. Read http://adv-r.had.co.nz/Data-structures.html for more information. The two most basic types of data structures are vectors and lists. Vectors Atomic Vector (numeric, character, integer, logical) All elements of an atomic vector must have the same type (numeric/double, character, integer, logical/boolean). x &lt;- 1:10 typeof(x) # atomic type class(x) # object class name is.integer(x) x x[1:2] #access elements in vector with [] z &lt;- (x &gt; 5) typeof(z) class(z) is.logical(z) z z[-c(1:2)] y &lt;- if_else(x &gt; 5, &#39;greater than 5&#39;,&#39;less than or equal to 5&#39;) typeof(y) class(y) is.character(y) y y[-c(1,5)] v &lt;- x + .2 typeof(v) class(v) is.numeric(v) is.double(v) Factors are vectors with special attributes. yfactor &lt;- factor(y) yfactor %&gt;% attributes() as.numeric(yfactor) #underlying a factor is an integer vector, indexing the levels/categories levels(yfactor) # first level is reference level relevel(yfactor, ref=&#39;less than or equal to 5&#39;) # can change reference level Lists Lists Lists can have elements of differing types. You can even have lists of lists or lists of vectors. l1 &lt;- list(x,y,z,v,2) #unnamed list l1 l1[[1]] #access elements in list with [[]] l2 &lt;- list(x = x, y = y, z = z, v = v, b = 2) #named list l2 l2[[&#39;x&#39;]] #access elements in list with [[]] l2$x #access elements in named list with $ Data frames and tibbles are special types of named lists. Each element must be a vector, the vectors must have the same length, but they can be different types. Each of these vectors are variables in the data set. You could have a list of data frames/tibbles. Map When you want to apply a function to each element in a list or vector of values, you can use map which is a function in the purrr package. # Silly example: add 2 to every value in x a &lt;- 1:10 map(a, ~ .x + 2 ) # short hand for code below # map(x, function(x) x + 2 ) # Real example: fit the same model to 5 different data sets #simulate data x &lt;- runif(500,0,10) #500 x values randomly from uniform distribution [0,10] df &lt;- data.frame(x = x, y = 2*x + rnorm(500), #500 y values equal to 2*x + random value from Normal(0,1) s = sample(size = 500, 1:5, replace=TRUE)) %&gt;% #500 random values from 1 to 5 (strata/groups) group_by(s) %&gt;% tidyr::nest() # data frame with variable data that is 5 data sets, one for each group map(df$data, ~ lm(y ~ x, data = .x)) # short hand for code below # map(df$data, function(data) lm(y ~ x, data = data)) Tidymodels Functions library(tidymodels) # Includes the workflows package tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions Model Specification For more information, see the documentation for the parsnip package (part of the tidymodels package): https://www.tidymodels.org/find/parsnip/ Types of Models For this course, we will only use the following models: linear_reg(), gen_additive_model(), logistic_reg() nearest_neighbor(), decision_tree(), rand_forest() Engines (Estimation Methods) For this course, we will primarily use the following estimation methods/engines. lm, glm, glmnet, kknn, rpart, ranger Usage Regression Models # Linear Regression Model lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) # Lasso Regression Model lm_lasso_spec &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = lambda) %&gt;% # we could let penalty = tune() set_engine(engine = &#39;glmnet&#39;) %&gt;% set_mode(&#39;regression&#39;) # Ridge Regression Model lm_ridge_spec &lt;- linear_reg() %&gt;% set_args(mixture = 0, penalty = lambda) %&gt;% # we could let penalty = tune() set_engine(engine = &#39;glmnet&#39;) %&gt;% set_mode(&#39;regression&#39;) # K Nearest Neighbors Regression Model knn_spec &lt;- nearest_neighbor() %&gt;% set_args(neighbors = k) %&gt;% # we could let neighbors = tune() set_engine(engine = &#39;kknn&#39;) %&gt;% set_mode(&#39;regression&#39;) # Generalized Additive Models (with smoothing splines) gam_spec &lt;- gen_additive_mod() %&gt;% set_engine(engine = &#39;mgcv&#39;) %&gt;% set_mode(&#39;regression&#39;) # Regression Tree Model rt_spec &lt;- decision_tree() %&gt;% set_args(tree_depth, min_n, cost_complexity) %&gt;% # we could let any of these = tune() set_engine(engine = &#39;rpart&#39;) %&gt;% set_mode(&#39;regression&#39;) # Random Forest Model for Regression rf_spec &lt;- rand_forest() %&gt;% set_args(mtry, min_n, trees) %&gt;% # we could let any of these = tune() set_engine(engine = &#39;ranger&#39;) %&gt;% set_mode(&#39;regression&#39;) Classification Models # Logistic Regression Model logistic_spec &lt;- logistic_reg() %&gt;% set_engine(engine = &#39;glm&#39;) %&gt;% set_mode(&#39;classification&#39;) # Lasso Logistic Regression Model logistic_lasso_spec &lt;- logistic_reg() %&gt;% set_args(mixture = 1, penalty = lambda) %&gt;% #we could let penalty = tune() set_engine(engine = &#39;glmnet&#39;) %&gt;% set_mode(&#39;classification&#39;) # K Nearest Neighbors Regression Model knn_spec &lt;- nearest_neighbor() %&gt;% set_args(neighbors = k) %&gt;% # we could let neighbors = tune() set_engine(engine = &#39;kknn&#39;) %&gt;% set_mode(&#39;classification&#39;) # Classification Tree Model ct_spec &lt;- decision_tree() %&gt;% set_args(tree_depth, min_n, cost_complexity) %&gt;% set_engine(engine = &#39;rpart&#39;) %&gt;% set_mode(&#39;classification&#39;) # Random Forest Model for Classification rf_class_spec &lt;- rand_forest() %&gt;% set_args(mtry, min_n, trees) %&gt;% # we could let any of these = tune() set_engine(engine = &#39;ranger&#39;) %&gt;% set_mode(&#39;classification&#39;) Preprocessing Data For more information and many examples, see the documentation for the recipes package (part of the tidymodels package): https://recipes.tidymodels.org/ Recipes To facilitate transparency, organization, and consistency of process, we create a recipe of the steps taken for preprocessing the data prior to modeling. rec &lt;- recipe(formula, data) %&gt;% step_{FILLIN}() %&gt;% step_{FILLIN}() # continue to add preprocessing steps; note that the order matters! Make sure to include the steps in the same order as they listed below. Row Operations Imputation Factor Levels Univariate Quant. Transformations Discretization Dummy Variables Interactions Normalization Multivariate Transformations Down/Up Sampling (we won’t talk about this very much) Row Operations The step functions available for row operations are below. step_filter(condition) # use rows that satisfy a condition step_sample(size) # use randomly select rows step_shuffle(var) # randomly change the order of rows for selected variables (not needed for most circumstances) step_slice(index) # use rows with the index (row numbers) provided step_arrange(var) # sort rows according to the values of selected variables step_corr() # removes predictors with high correlation with other predictors step_lincomb() # removes predictors that are linear combinations of others step_zv() # removes predictors with zero variability step_nzv() # removes predictors with near zero variability Imputation The step functions available for imputing values for missing values are below. step_impute_bag(impute_with) # imputation via bagged trees step_impute_knn(impute_with) # imputation via K-nearest neighbors step_impute_mode() # imputation of most common value step_unknown() # assign missing to &quot;unknown&quot; category Factor Levels The step functions available for changing the levels for factors are below. step_relevel(ref_level) # change reference level of factor Univariate Quant. Transformations The step functions available for transforming quantitative variables are below. step_BoxCox() # box cox transformation step_log() # log transformation step_bs() # basis spline step_ns() # natural spline step_poly() # polynomial transformation step_mutate() # mutate a variable in the way you specify Discretization The step functions available for transforming quantitative variables into categorical variables are below. step_discretize(num_breaks) step_cut(breaks) Dummy Variables The step functions available for transforming categorical variables into quantitative variables (dummy/indicator variables) and dealing with categorical levels are below. step_novel() # create category for new levels in test set step_other(threshold) # collapse small categories into an &quot;other&quot; category step_date(features = c(&#39;dow&#39;,&#39;month&#39;,&#39;year&#39;)) # create features based on date variables step_dummy() # create indicators based on nominal variables step_regex() # create indicators using regular expressions Interactions The step functions available for creating interaction terms are below. step_interact(~ A:B) step_interact(~ starts_with(&quot;CatA&quot;):starts_with(&quot;CatB&quot;)) Normalization The step functions available for centering, scaling, and standardizing/normalizing are below. step_center() # subtracts mean step_scale() # divides by sd step_normalize() # both center and scale Multivariate Transformations The step functions available for combining/transforming many quantitative variables are below. step_pca() Variable Roles In a recipe, each variable has a role. Typical roles are: Outcome Predictor # Use summary to see the roles of the variables recipe() %&gt;% summary() # in the steps of the recipe, you can refer to variables based on their roles all_predictors() all_outcomes() # in the recipe, you can refer to variables based on their type (quantitative: numeric, categorical: nominal) all_numeric() all_nominal() # in the recipe, you can refer to variables based on their type and role all_numeric_predictors() all_nominal_predictors() # roles can help you select groups of variables has_role() has_type() Roles to consider adding if you want to keep variables in the data set but do not want to use them in the model: ID (to exclude from modeling) Locations Date/Times General Categories of Predictors rec &lt;- recipe() %&gt;% update_role() rec &lt;- recipe() %&gt;% add_role() %&gt;% # variables can have multiple roles remove_role() Prep Data If you aren’t ready to fit the model but want to see the pre-processed data, you can use these functions. Note: The prep and bake steps are done for you when you fit models recipe() %&gt;% prep(training = train_data) %&gt;% juice() # returns preprocessed training data recipe() %&gt;% prep(training = train_data) %&gt;% bake(new_data = test_data) # returns preprocessed test data using values from train data Data Subsets For more information and many examples, see the documentation for the rsample package (part of the tidymodels package): https://rsample.tidymodels.org/ Random Splits To randomly split your data set into a training and test set, use initial_split(). You can change prop to 0.8 for a training set equal to 80% of the rows. You can specify a variable for the strata argument to make the random sampling to be conducted within the stratification variable; this can help ensure that the number of data points in the training data is equivalent to the proportions in the original data set. set.seed(123) # to reproduce your random split, set a seed - pick an integer data_split &lt;- initial_split(data, prop = 3/4) # saves minimal information about the split (which rows are in the training set and which are in the testing set) data_train &lt;- training(data_split) data_test &lt;- testing(data_split) Cross-Validation/Resamples To complete cross-validation, use vfold_cv(). This function saves information about the folds (which rows are in which fold) and which rows should be used for training and testing for each iteration. data_cv &lt;- vfold_cv(data, v = 10) # In practice, you don&#39;t need to use the following functions but it is useful to know what is going on behind the scenes training(data_cv$splits[[1]]) # pulls training data for the 1st split (1st fold is testing set) testing(data_cv$splits[[1]]) # pulls testing data for the 1st split (1st fold is testing set) Workflows A machine learning workflow (the “black box”) = model specification + preprocessing recipe/formula model_wf &lt;- workflow() %&gt;% add_recipe(rec) %&gt;% # add_formula(formula) if no recipe add_model(mod_spec) model_wf %&gt;% update_recipe(rec_new) model_wf %&gt;% update_formula(formula_new) model_wf %&gt;% update_model(model_spec_new) Fit Model # Fit model to given training data fit_model &lt;- fit(model_spec, formula, data_train) # fit model using a formula (no recipe) and model_spec fit_model &lt;- fit(model_wf, data_train) # fit model using a workflow # Fit model using CV fit_model_cv &lt;- fit_resamples(model_wf, # fit and evaluate model with CV resamples = data_cv, metrics = metric_set() ) # from tune package # Fit model on one training and evaluate on one test data fit_model_traintest &lt;- last_fit(model_wf, # fit model on training and evaluate model on test data split = data_split, metrics = metric_set() ) # from tune package Evaluate Model: Metrics For documentation and examples about metrics, see the yardstick package, https://yardstick.tidymodels.org/index.html. metric_set(rmse, rsq, mae) # typical regression metrics metric_set(sens, spec, accuracy, roc_auc) # typical classification metrics For documentation and examples about collecting metrics from train/testing or cross validation, see the workflowsets package, https://workflowsets.tidymodels.org/ # Using one training and testing split last_fit(model_wf, split = data_split, metrics = metric_set(rmse, rsq, mae) ) %&gt;% collect_metrics() # Using CV fit_resamples(model_wf, resamples = data_cv, metrics = metric_set(rmse, rsq, mae) ) %&gt;% collect_metrics(summarize = TRUE) Tune Model: Choosing tuning parameters For documentation and examples about tuning a model by selecting tuning parameters, see the tune package, https://tune.tidymodels.org/ Use tune() for tuning parameters in model and recipe and tune_grid() to fit models with an array of values for the tuning parameters. # Regularized Regression (LASSO, Ridge) lm_lasso_spec_tune &lt;- lm_lasso_spec %&gt;% set_args(mixture = 1, penalty = tune(&#39;lambda&#39;)) lm_ridge_spec_tune &lt;- lm_ridge_spec %&gt;% set_args(mixture = 0, penalty = tune(&#39;lambda&#39;)) # Number of neighbors in KNN knn_spec_tune &lt;- knn_spec %&gt;% set_args(neighbors = tune(&#39;k&#39;)) # Decision Trees rt_spec_tune &lt;- rt_spec %&gt;% set_args(cost_complexity = tune(&#39;cost&#39;), min_n = tune(&#39;n&#39;)) ct_spec_tune &lt;- ct_spec %&gt;% set_args(cost_complexity = tune(&#39;cost&#39;), min_n = tune(&#39;n&#39;)) # Number of Components in PCA recipe() %&gt;% step_pca(num_comp = tune(&#39;n&#39;)) Once you’ve included tune() as the input for the parameter in the model or recipe, you need to fit the model for a variety of values. You may provide a grid of values or you can let the function create a grid of a specified number of values. tune_results &lt;- tune_grid(model_wf, resamples = data_cv, grid = 10, #creates grid of 10 values metrics = metric_set()) tune_results&lt;- tune_grid(model_wf, resamples = data_cv, grid = expand_grid(k = seq(0,15,by = 1)), #you provide grid, use name in tune() for variable name metrics = metric_set()) tune_results %&gt;% show_best() tune_results %&gt;% autoplot() final_model &lt;- select_best(tune_results, metric) # or select_by_one_std_err(tune_results, metric, desc(penalty)) final_model_wf &lt;- finalize_workflow(model_wf, final_model) Use Final Model: Predictions # Fit final model on training data and then predict for any new data set final_model_wf %&gt;% fit(data = data_train) %&gt;% predict(new_data = data_test) # get .pred only # Fit final model on training data and then predict for test set final_model_wf %&gt;% last_fit( split = data_split, metrics = metric_set(rmse, rsq, mae) ) %&gt;% collect_predictions() # get .pred and true values in test set Extraction For some models (LASSO, decision trees), you’ll want to pull out the original fit object to make visualizations. fit_model %&gt;% extract_fit_engine() # gives you the original fit object by the engine (useful for specific purposes) pluck(list, &#39;fit&#39;) # gives you the element in the list associated with the name &#39;fit&#39; Variable Importance Model Agnostic Importance - Permutation Approach Permutation technique - Train model - Evaluate model For each variable, Randomly shuffle the variable and reevaluate model Compare the original model evaluation metric to the value when the variable is reshuffled Reshuffling the values of a variable breaks or removes any predictive relationships that may exist with that variable and the outcome variable. So if a variable is highly predictive of the outcome, there should be a large change in the model evaluation metric when it is “removed” through reshuffling. library(vip) conflicted::conflict_prefer(&quot;vi&quot;, &quot;vip&quot;) train_prep &lt;- train %&gt;% prep() %&gt;% juice() # Pre-process the training data mod &lt;- fit_model %&gt;% extract_fit_engine() # Pull the fit model # Calculate Variable Importance using Permutation approach vi(mod, method = &#39;permute&#39;, target = &#39;outcomename&#39;, metric = &#39;metricname&#39;, train = train_prep, pred_wrapper = predict) # use vip instead of vi for plot Model Specific Importance ## LASSO Var Importance glmnet_output &lt;- final_fit_se %&gt;% extract_fit_engine() # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- glmnet_output$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { this_coeff_path &lt;- bool_predictor_exclude[row,] if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{ return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)} }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) ## Tree and Forest Var Importance library(vip) conflicted::conflict_prefer(&quot;vi&quot;, &quot;vip&quot;) tree &lt;- fit_tree_model %&gt;% extract_fit_engine() # fit_tree_model would come from fitting a decision_tree() model vi(tree) rf &lt;- fit_rf_model %&gt;% extract_fit_engine() # fit_rf_model would come from fitting a rand_forest() model vi(rf) # use vip instead of vi for plot Comparing Models: Workflow Sets For documentation and examples about comparing models, see the workflowsets package, https://workflowsets.tidymodels.org/ wf_set &lt;- workflow_set( preproc = list(rec1 = recipe1, rec2 = recipe2, rec3 = recipe3), models = list(mod1 = model_spec1, mod2 = model_spec2, mod3 = model_spec3), cross = TRUE) %&gt;% workflow_map( &quot;fit_resamples&quot;, resamples, metrics) wf_set %&gt;% autoplot() wf_set %&gt;% rank_results(rank_metric) Visualizing Fit Models Visualize Coefficients # For Linear Regression fit_model %&gt;% tidy() %&gt;% slice(-1) %&gt;% mutate(lower = estimate - 1.96*std.error, upper = estimate + 1.96*std.error) %&gt;% ggplot() + geom_vline(xintercept=0, linetype=4) + geom_point(aes(x=estimate, y=term)) + geom_segment(aes(y=term, yend=term, x=lower, xend=upper), arrow = arrow(angle=90, ends=&#39;both&#39;, length = unit(0.1, &#39;cm&#39;))) + labs(x = &#39;Coefficient estimate (95% CI)&#39;, y = &#39;Feature&#39;) + theme_classic() # For Logistic Regression fit_model %&gt;% tidy() %&gt;% slice(-1) %&gt;% mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %&gt;% # do this first mutate(OR = exp(estimate)) %&gt;% ggplot() + geom_vline(xintercept=1, linetype=4) + geom_point(aes(x=OR, y=term)) + geom_segment(aes(y=term, yend=term, x=OR.conf.lower, xend=OR.conf.upper), arrow = arrow(angle=90, ends=&#39;both&#39;, length = unit(0.1, &#39;cm&#39;))) + labs(x = &#39;Coefficient estimate (95% CI)&#39;, y = &#39;Feature&#39;) + theme_classic() Visualize Effects # Linear Regression data %&gt;% mutate(id = row_number()) %&gt;% pivot_longer(-c(outcome,id), names_to = &#39;key&#39;, values_to = &#39;value&#39;) %&gt;% right_join( (lm_fit_model %&gt;% tidy() %&gt;% slice(-1) %&gt;% select(term, estimate)), by = c(&#39;key&#39;=&#39;term&#39;) ) %&gt;% mutate(effect = value * estimate) %&gt;% ggplot(aes(x = effect, y = key)) + geom_boxplot() + geom_vline(xintercept = 0, color = &#39;grey&#39;) + labs(x = &#39;Effect&#39;, y = &#39;Feature&#39;) + theme_classic() # Logistic Regression data %&gt;% mutate(id = row_number()) %&gt;% pivot_longer(-c(outcome,id), names_to = &#39;key&#39;, values_to = &#39;value&#39;) %&gt;% right_join( (glm_fit_model %&gt;% tidy() %&gt;% slice(-1) %&gt;% select(term, estimate)), by = c(&#39;key&#39;=&#39;term&#39;) ) %&gt;% mutate(effect = value * estimate) %&gt;% ggplot(aes(x = effect, y = key)) + geom_boxplot() + geom_vline(xintercept = 0, color = &#39;grey&#39;) + labs(x = &#39;Effect (Log Odds units)&#39;, y = &#39;Feature&#39;) + theme_classic() Visualize Residuals # For Any Regression Model #Predicted vs. Residual fit_model %&gt;% predict(new_data = data_train) %&gt;% bind_cols(data_train) %&gt;% mutate(resid = outcome - .pred) %&gt;% ggplot(aes(x = .pred, y = resid)) + geom_point() + geom_smooth() + theme_classic() #Variable vs. Residual fit_model %&gt;% predict(new_data = data_train) %&gt;% bind_cols(data_train) %&gt;% mutate(resid = outcome - .pred) %&gt;% ggplot(aes(x = var, y = resid)) + geom_point() + geom_smooth() + theme_classic() Visualize Predictions # For Any Regression Model # Create a data_grid (fixing all other variables at some value, sequence of values for the variable of interest) data_grid &lt;- data.frame(x1 = value1, x2 = value2, x3 = value3, x4 = seq(0,100,by = 1)) #example where x4 is variable of interest # Get Predicted Values for test values in data_grid predicted_lines &lt;- data_grid %&gt;% bind_cols( predict(fit_model, new_data = data_grid), predict(fit_model, new_data = data_grid, type=&#39;conf_int&#39;) ) train_data %&gt;% ggplot(aes(x = x4, y = outcome)) + geom_point() + geom_line(aes(y = .pred, x = x4), data = predicted_lines) + geom_line(aes(y = .pred_lower, x = x4), data = predicted_lines) + geom_line(aes(y = .pred_upper, x = x4), data = predicted_lines) + theme_classic() 0.0.1 Visualize LASSO Coefficients glmnet_output &lt;- final_fit_se %&gt;% extract_fit_engine() # get the original glmnet output lambdas &lt;- glmnet_output$lambda coefs_lambdas &lt;- coefficients(glmnet_output, s = lambdas ) %&gt;% as.matrix() %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(lambda = lambdas ) %&gt;% select(lambda, everything(), -`(Intercept)`) %&gt;% pivot_longer(cols = -lambda, names_to = &quot;term&quot;, values_to = &quot;coef&quot;) %&gt;% mutate(var = purrr::map_chr(stringr::str_split(term,&quot;_&quot;),~.[1])) coefs_lambdas %&gt;% ggplot(aes(x = lambda, y = coef, group = term, color = var)) + geom_line() + geom_vline(xintercept = best_se_penalty %&gt;% pull(penalty), linetype = &#39;dashed&#39;) + theme_classic() + theme(legend.position = &quot;bottom&quot;, legend.text=element_text(size=8)) Real Data Example: Regression library(tidymodels) tidymodels_prefer() data(ames) set.seed(123) ames &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price)) # If you do one train/test split data_split &lt;- initial_split(ames, strata = &quot;Sale_Price&quot;, prop = 0.75) #Create Train/Test set ames_train &lt;- training(data_split) # Fit model to this ames_test &lt;- testing(data_split) # Don&#39;t use until evaluating final model Linear Regression lm_spec &lt;- linear_reg() %&gt;% # Specify Model and Engine set_engine( engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) lm_rec &lt;- recipe(Sale_Price ~ Lot_Area + Year_Built + House_Style + Gr_Liv_Area + Fireplaces, data = ames_train) %&gt;% step_lincomb(all_numeric_predictors()) %&gt;% # Specify Formula and Preprocessing Recipe step_zv(all_numeric_predictors()) %&gt;% step_mutate(Gr_Liv_Area = Gr_Liv_Area/100, Lot_Area = Lot_Area/100) %&gt;% step_mutate(Fireplaces = Fireplaces &gt; 0) %&gt;% step_cut(Year_Built, breaks = c(0, 1950, 1990, 2020)) %&gt;% step_other(House_Style,threshold = .1) %&gt;% step_dummy(all_nominal_predictors()) train_prep &lt;- lm_rec %&gt;% prep() %&gt;% juice() # Pre-process Training Data ames_wf &lt;- workflow() %&gt;% # Create Workflow (Recipe + Model Spec) add_recipe(lm_rec) %&gt;% add_model(lm_spec) lm_fit_train &lt;- ames_wf %&gt;% fit(data = ames_train) # Fit Model to Training Data train_prep %&gt;% select(Sale_Price) %&gt;% bind_cols( predict(lm_fit_train, ames_train) ) %&gt;% metrics(estimate = .pred, truth = Sale_Price) # Calculate Training metrics lm_fit_train %&gt;% tidy() # Model Coefficients from Trained Model library(dotwhisker) tidy(lm_fit_train) %&gt;% # Viz of Trained Model Coef dwplot(dot_args = list(size = 2, color = &quot;black&quot;), whisker_args = list(color = &quot;black&quot;), vline = geom_vline(xintercept = 0, color = &quot;grey50&quot;, linetype = 2)) ames_cv &lt;- vfold_cv(ames_train, v = 10, strata = Sale_Price) # Create 10 Folds of Training Data for CV lm_fit_cv &lt;- fit_resamples(ames_wf, # Fit Model to 10 Folds of Training Data resamples = ames_cv, metrics = metric_set(rmse, mae, rsq)) lm_fit_cv %&gt;% collect_metrics() # Evaluate Trained Model using CV lm_fit_test &lt;- last_fit(ames_wf, split = data_split) lm_fit_test %&gt;% collect_metrics() #Evaluation on Test Data library(vip) conflicted::conflict_prefer(&quot;vi&quot;, &quot;vip&quot;) mod &lt;- lm_fit_train %&gt;% extract_fit_engine() vi(mod, method = &#39;permute&#39;, target = &#39;Sale_Price&#39;, metric = &#39;rmse&#39;, train = train_prep, pred_wrapper = predict) vip(mod, method = &#39;permute&#39;, target = &#39;Sale_Price&#39;, metric = &#39;rmse&#39;, train = train_prep, pred_wrapper = predict) + theme_classic() Regularized Regression lm_lasso_spec &lt;- linear_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% set_engine(engine = &#39;glmnet&#39;) %&gt;% set_mode(&#39;regression&#39;) # Update Workflow (Recipe + Model Spec) ames_wf &lt;- ames_wf %&gt;% update_model(lm_lasso_spec) # Tune Model by Fitting Model to 10 Folds of Training Data lasso_fit_cv &lt;- tune_grid(ames_wf, resamples = ames_cv, grid = 10, metrics = metric_set(rmse, mae, rsq)) lasso_fit_cv %&gt;% autoplot() + theme_classic() # Evaluate Trained Model using CV # Select penalty value lasso_fit_cv %&gt;% show_best(metric = &#39;rmse&#39;) best_penalty &lt;- lasso_fit_cv %&gt;% select_by_one_std_err(metric = &#39;rmse&#39;,desc(penalty)) tuned_ames_wf &lt;- finalize_workflow(ames_wf,best_penalty) lm_lasso_spec &lt;- tuned_ames_wf %&gt;% pull_workflow_spec() # Save final tuned model spec # CV metrics lasso_fit_cv %&gt;% collect_metrics() %&gt;% filter(penalty == (best_penalty %&gt;% pull(penalty))) # Fit Tuned Lasso Model to Training Data lasso_fit_train &lt;- tuned_ames_wf %&gt;% fit(data = ames_train) # Training metrics train_prep %&gt;% select(Sale_Price) %&gt;% bind_cols( predict(lasso_fit_train, ames_train) ) %&gt;% metrics(estimate = .pred, truth = Sale_Price) # Model Coefficients from Trained Model lasso_fit_train %&gt;% tidy() # Var Importance glmnet_output &lt;- lasso_fit_train %&gt;% extract_fit_engine() # Create a boolean matrix (predictors x lambdas) of variable exclusion bool_predictor_exclude &lt;- glmnet_output$beta==0 # Loop over each variable var_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) { this_coeff_path &lt;- bool_predictor_exclude[row,] if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{ return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)} }) # Create a dataset of this information and sort var_imp_data &lt;- tibble( var_name = rownames(bool_predictor_exclude), var_imp = var_imp ) var_imp_data %&gt;% arrange(desc(var_imp)) Nearest Neighbors knn_spec &lt;- nearest_neighbor() %&gt;% set_args(neighbors = tune()) %&gt;% set_engine(engine = &#39;kknn&#39;) %&gt;% set_mode(&#39;regression&#39;) # Update Workflow (Recipe + Model Spec) ames_wf &lt;- ames_wf %&gt;% update_model(knn_spec) # Tune Model by Fitting Model to 10 Folds of Training Data knn_fit_cv &lt;- tune_grid(ames_wf, resamples = ames_cv, grid = 20, metrics = metric_set(rmse, mae, rsq)) knn_fit_cv %&gt;% autoplot() # Evaluate Trained Model using CV # Select penalty value knn_fit_cv %&gt;% show_best(metric = &#39;rmse&#39;) best_neighbor &lt;- knn_fit_cv %&gt;% select_by_one_std_err(metric = &#39;rmse&#39;, neighbors) tuned_ames_wf &lt;- finalize_workflow(ames_wf, best_neighbor) knn_spec &lt;- tuned_ames_wf %&gt;% pull_workflow_spec() # Save final tuned model spec # CV Metrics knn_fit_cv %&gt;% collect_metrics() %&gt;% filter(neighbors == (best_neighbor %&gt;% pull(neighbors))) # Fit KNN Model to Training Data knn_fit_train &lt;- tuned_ames_wf %&gt;% fit(data = ames_train) train_prep %&gt;% select(Sale_Price) %&gt;% bind_cols( predict(knn_fit_train, ames_train) ) %&gt;% metrics(estimate = .pred, truth = Sale_Price) # Training metrics # Visualize Predictions data_grid &lt;- data.frame(Lot_Area = median(ames_train$Lot_Area),Year_Built = median(ames_train$Year_Built), House_Style = &#39;One_Story&#39;, Fireplaces = 1, Gr_Liv_Area = seq(300,5000,by = 100)) # Get Predicted Values for test values in data_grid predicted_lines &lt;- data_grid %&gt;% bind_cols( predict(knn_fit_train , new_data = data_grid) ) ames_train %&gt;% ggplot(aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point() + geom_line(aes(y = .pred, x = Gr_Liv_Area), data = predicted_lines, color = &#39;red&#39;) + theme_classic() GAMs - Splines and Local Regression spline_rec &lt;- lm_rec %&gt;% step_ns(Gr_Liv_Area, deg_free = 4) ames_wf &lt;- ames_wf %&gt;% # Update Workflow (Recipe + Model Spec) update_recipe(spline_rec) %&gt;% update_model(lm_spec) spline_fit_train &lt;- ames_wf %&gt;% fit(data = ames_train) # Fit Spline Model to Training Data train_prep %&gt;% select(Sale_Price) %&gt;% bind_cols( predict(spline_fit_train, ames_train) ) %&gt;% metrics(estimate = .pred, truth = Sale_Price) # Training metrics spline_fit_cv &lt;- ames_wf %&gt;% fit_resamples(resamples = ames_cv, metrics = metric_set(rmse, mae, rsq)) # CV Metrics spline_fit_cv %&gt;% collect_metrics() # Visualize Predictions (Non-Linear Functions) data_grid &lt;- data.frame(Lot_Area = median(ames_train$Lot_Area),Year_Built = median(ames_train$Year_Built), House_Style = &#39;One_Story&#39;, Fireplaces = 1, Gr_Liv_Area = seq(300,5000,by = 100)) # Get Predicted Values for test values in data_grid predicted_lines &lt;- data_grid %&gt;% bind_cols( predict(spline_fit_train , new_data = data_grid) ) ames_train %&gt;% ggplot(aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point() + geom_line(aes(y = .pred, x = Gr_Liv_Area), data = predicted_lines, color = &#39;red&#39;) + theme_classic() GAM - Smoothing Splines gam_spec &lt;- gen_additive_mod() %&gt;% set_engine(engine = &#39;mgcv&#39;) %&gt;% set_mode(&#39;regression&#39;) fit_gam_model &lt;- gam_spec %&gt;% fit(Sale_Price ~ Lot_Area + Year_Built + House_Style + s(Gr_Liv_Area, k = 15) + Fireplaces, data = ames_train) # Summary: Parameter (linear) estimates and then Smooth Terms (H0: no relationship) fit_gam_model %&gt;% pluck(&#39;fit&#39;) %&gt;% summary() # Diagnostics: Check to see if the number of knots is large enough (if p-value is low, increase number of knots) par(mfrow=c(2,2)) fit_gam_model %&gt;% pluck(&#39;fit&#39;) %&gt;% mgcv::gam.check() # Visualize Non-Linear Functions fit_gam_model %&gt;% pluck(&#39;fit&#39;) %&gt;% plot() Regression Trees tree_rec &lt;- recipe(Sale_Price ~ Lot_Area + Year_Built + House_Style + Gr_Liv_Area + Fireplaces, data = ames_train) %&gt;% step_zv(all_numeric_predictors()) tree_spec &lt;- decision_tree() %&gt;% set_engine(engine = &#39;rpart&#39;) %&gt;% set_mode(&#39;regression&#39;) ames_wf &lt;- ames_wf %&gt;% # Update Workflow (Recipe + Model Spec) update_recipe(tree_rec) %&gt;% update_model(tree_spec) tree_fit_train &lt;- ames_wf %&gt;% fit(data = ames_train) # Fit Reg Tree Model to Training Data train_prep %&gt;% select(Sale_Price) %&gt;% bind_cols( predict(tree_fit_train, ames_train) ) %&gt;% metrics(estimate = .pred, truth = Sale_Price) # Training metrics tree_fit_train %&gt;% extract_fit_engine() %&gt;% rpart.plot::rpart.plot(roundint = FALSE) tree_fit_cv &lt;- ames_wf %&gt;% fit_resamples(resamples = ames_cv, metrics = metric_set(rmse, mae, rsq)) tree_fit_cv %&gt;% collect_metrics() Comparison of Methods wf_set &lt;- workflow_set( preproc = list(lm = lm_rec, spline = spline_rec, tree = tree_rec), models = list(lm = lm_spec, lasso = lm_lasso_spec, knn = knn_spec, tree = tree_spec), cross = TRUE) %&gt;% anti_join(tibble(wflow_id = c(&quot;lm_tree&quot;,&quot;spline_lasso&quot;, &quot;spline_knn&quot;,&quot;spline_tree&quot;,&quot;tree_lm&quot;,&quot;tree_lasso&quot;,&quot;tree_knn&quot;)), by = &quot;wflow_id&quot;) ames_wf_set &lt;- wf_set %&gt;% workflow_map( &quot;fit_resamples&quot;, resamples = ames_cv, # Compare Methods via CV metrics = metric_set(rmse, rsq, mae)) ames_wf_set %&gt;% autoplot() ames_wf_set %&gt;% rank_results(rank_metric = &#39;rmse&#39;) list(lm = lm_fit_test, lasso = lasso_fit_test, knn = knn_fit_test, spline = spline_fit_test, tree = tree_fit_test) %&gt;% # Comparison of Methods fit to Test Data map_df(collect_metrics, .id = &#39;Model&#39;) %&gt;% ggplot(aes(x = Model, y = .estimate)) + geom_point() + facet_grid(.metric ~ ., scales = &#39;free&#39;) + theme_classic() new_ames &lt;- crossing(Fireplaces = 0:1, Lot_Area = seq(1500, 15000, length = 10), Gr_Liv_Area = seq(500, 5000, length = 10), Year_Built = c(1900,1950, 1975,2000), House_Style = c(&#39;One_Story&#39;, &#39;Two_Story&#39;,&#39;SLvl&#39;)) %&gt;% filter(!(Gr_Liv_Area &gt; Lot_Area)) all_preds &lt;- new_ames %&gt;% bind_cols(predict(lm_fit_train,new_ames)) %&gt;% rename(.lm.pred = .pred) %&gt;% bind_cols(predict(lasso_fit_train,new_ames)) %&gt;% rename(.lasso.pred = .pred) %&gt;% bind_cols(predict(knn_fit_train,new_ames)) %&gt;% rename(.knn.pred = .pred) %&gt;% bind_cols(predict(spline_fit_train,new_ames)) %&gt;% rename(.spline.pred = .pred) %&gt;% bind_cols(predict(tree_fit_train,new_ames)) %&gt;% rename(.tree.pred = .pred) # Visualize Predictions from these Models all_preds %&gt;% filter(Year_Built == 1975 &amp; Fireplaces == 0) %&gt;% ggplot(aes(x = Gr_Liv_Area, y = .spline.pred, color = House_Style )) + geom_point() + geom_line() + facet_wrap(~Lot_Area) + theme_classic() Real Data Example: Classification library(tidymodels) tidymodels_prefer() data(ames) set.seed(123) ames &lt;- ames %&gt;% mutate(High_Sale_Price = factor(Sale_Price &gt; quantile(Sale_Price,.75))) data_split &lt;- initial_split(ames, strata = &quot;High_Sale_Price&quot;, prop = 0.75) #Create Train/Test set ames_train &lt;- training(data_split) # Fit model to this ames_test &lt;- testing(data_split) # Don&#39;t use until evaluating final model Logistic Regression logistic_spec &lt;- logistic_reg() %&gt;% # Specify Model and Engine set_engine( engine = &#39;glm&#39;) %&gt;% set_mode(&#39;classification&#39;) glm_rec &lt;- recipe(High_Sale_Price ~ Lot_Area + Year_Built + House_Style + Gr_Liv_Area + Fireplaces, data = ames_train) %&gt;% step_lincomb(all_numeric_predictors()) %&gt;% # Specify Formula and Preprocessing Recipe step_zv(all_numeric_predictors()) %&gt;% step_mutate(Gr_Liv_Area = Gr_Liv_Area/100, Lot_Area = Lot_Area/100) %&gt;% step_mutate(Fireplaces = Fireplaces &gt; 0) %&gt;% step_cut(Year_Built, breaks = c(0, 1950, 1990, 2020)) %&gt;% step_other(House_Style,threshold = .1) %&gt;% step_dummy(all_nominal_predictors()) train_prep &lt;- glm_rec %&gt;% prep() %&gt;% juice() # Create Prepped Training Data ames_wf &lt;- workflow() %&gt;% # Create Workflow (Recipe + Model Spec) add_recipe(glm_rec) %&gt;% add_model(logistic_spec) logistic_fit_train &lt;- ames_wf %&gt;% fit(data = ames_train) # Fit Model to Training Data predict(logistic_fit_train, ames_train) %&gt;% bind_cols(train_prep %&gt;% select(High_Sale_Price)) %&gt;% metrics(estimate = .pred_class, truth = High_Sale_Price) # Training metrics logistic_fit_train %&gt;% tidy() # Model Coefficients from Trained Model library(dotwhisker) tidy(logistic_fit_train) %&gt;% # Viz of Trained Model Odds Ratios mutate(conf.low = exp(estimate - 1.96*std.error),conf.high = exp(estimate + 1.96*std.error)) %&gt;% # do this first mutate(estimate = exp(estimate)) %&gt;% # this second dwplot(dot_args = list(size = 2, color = &quot;black&quot;), whisker_args = list(color = &quot;black&quot;), vline = geom_vline(xintercept = 1, color = &quot;grey50&quot;, linetype = 2)) + labs(x = &#39;Odds Ratio&#39;) + theme_classic() ames_cv &lt;- vfold_cv(ames_train, v = 10, strata = Sale_Price) # Create 10 Folds of Training Data for CV logistic_fit_cv &lt;- fit_resamples(ames_wf, # Fit Model to 10 Folds of Training Data resamples = ames_cv, metrics = metric_set(sens, spec, accuracy, roc_auc)) logistic_fit_cv %&gt;% collect_metrics() # Evaluate Model using CV logistic_fit_test &lt;- last_fit(ames_wf, split = data_split, metrics = metric_set(sens, spec, accuracy, roc_auc)) logistic_fit_test %&gt;% collect_metrics() #Evaluation on Test Data Regularized Logistic Regression logistic_lasso_spec &lt;- logistic_reg() %&gt;% set_args(mixture = 1, penalty = tune()) %&gt;% set_engine(engine = &#39;glmnet&#39;) %&gt;% set_mode(&#39;classification&#39;) ames_wf &lt;- ames_wf %&gt;% # Update Workflow (Recipe + Model Spec) update_model(logistic_lasso_spec) logistic_lasso_fit_cv &lt;- tune_grid(ames_wf, # Fit Model to 10 Folds of Training Data resamples = ames_cv, grid = 10, metrics = metric_set(sens, spec, accuracy, roc_auc)) logistic_lasso_fit_cv %&gt;% autoplot() # Evaluate Trained Model using CV logistic_lasso_fit_cv %&gt;% show_best(metric = &#39;roc_auc&#39;) tuned_ames_wf &lt;- logistic_lasso_fit_cv %&gt;% select_best(metric = &#39;roc_auc&#39;) %&gt;% finalize_workflow(ames_wf,.) #set penalty based on best fit logistic_lasso_spec &lt;- tuned_ames_wf %&gt;% pull_workflow_spec() # Save final tuned model spec logistic_lasso_fit_train &lt;- tuned_ames_wf %&gt;% fit(data = ames_train) # Fit Lasso Model to Training Data (penalty is now set) logistic_lasso_fit_train %&gt;% predict(new_data = ames_train) %&gt;% bind_cols(train_prep %&gt;% select(High_Sale_Price)) %&gt;% metrics(estimate = .pred_class, truth = High_Sale_Price) # Training metrics logistic_lasso_fit_train %&gt;% tidy() # Model Coefficients from Trained Model lasso_fit_test &lt;- last_fit(tuned_ames_wf, split = data_split, metrics = metric_set(sens, spec, accuracy, roc_auc)) lasso_fit_test %&gt;% collect_metrics() #Evaluation on Test Data Classification Trees ctree_spec &lt;- decision_tree() %&gt;% set_engine(engine = &#39;rpart&#39;) %&gt;% set_mode(&#39;classification&#39;) ames_wf &lt;- ames_wf %&gt;% # Update Workflow (Recipe + Model Spec) update_model(ctree_spec) ctree_fit_train &lt;- ames_wf %&gt;% fit(data = ames_train) # Fit Reg Tree Model to Training Data ctree_fit_train %&gt;% predict(ames_train) %&gt;% bind_cols(train_prep %&gt;% select(High_Sale_Price) ) %&gt;% metrics(estimate = .pred_class, truth = High_Sale_Price) # Training metrics ctree_fit_train %&gt;% pull_workflow_fit() %&gt;% pluck(&#39;fit&#39;) %&gt;% rpart.plot::rpart.plot(roundint = FALSE) ctree_fit_cv &lt;- ames_wf %&gt;% fit_resamples(resamples = ames_cv, metrics = metric_set(sens,spec,accuracy,roc_auc)) ctree_fit_cv %&gt;% collect_metrics() last_fit(ames_wf, split = data_split, metrics = metric_set(sens,spec,accuracy,roc_auc) ) %&gt;% collect_metrics() # Evaluation on Test Data Random Forest rf_spec &lt;- rand_forest() %&gt;% set_engine(engine = &#39;ranger&#39;) %&gt;% set_mode(&#39;classification&#39;) ames_wf &lt;- ames_wf %&gt;% # Update Workflow (Recipe + Model Spec) update_model(rf_spec) rf_fit_train &lt;- ames_wf %&gt;% fit(data = ames_train) # Fit Reg Tree Model to Training Data rf_fit_train %&gt;% predict(ames_train) %&gt;% bind_cols(train_prep %&gt;% select(High_Sale_Price) ) %&gt;% metrics(estimate = .pred_class, truth = High_Sale_Price) # Training metrics rf_fit_cv &lt;- ames_wf %&gt;% fit_resamples(resamples = ames_cv, metrics = metric_set(sens,spec,accuracy,roc_auc)) rf_fit_cv %&gt;% collect_metrics() last_fit(ames_wf, split = data_split, metrics = metric_set(sens,spec,accuracy,roc_auc) ) %&gt;% collect_metrics() # Evaluation on Test Data Comparison of Methods wf_set &lt;- workflow_set( preproc = list(glm_rec), models = list(logistic = logistic_spec, lasso = logistic_lasso_spec, tree = ctree_spec, rf = rf_spec), cross = TRUE) ames_wf_set &lt;- wf_set %&gt;% workflow_map( &quot;fit_resamples&quot;, resamples = ames_cv, # Compare Methods via CV metrics = metric_set(accuracy, sens, spec, roc_auc )) ames_wf_set %&gt;% autoplot() ames_wf_set %&gt;% rank_results(rank_metric = &#39;roc_auc &#39;) Real Data Example: Clustering Real Data Example: Dimension Reduction "],["introductions.html", "Topic 1 Introductions Envisioning a Community of Learners Explorations", " Topic 1 Introductions Slides from today are available here. Envisioning a Community of Learners Directions: In small groups, please first introduce yourselves in whatever way you feel appropriate (e.g. name, pronouns, how you’re feeling at the moment, things you’re looking forward to, best part of summer, why you are motivated to take this class). When everyone is ready, discuss the prompts below. One of you volunteer to record a few thoughts in this Google Doc. The instructor will summarize responses from all the sections to create a resource that everyone can use. Prompts: Collectivist education focuses on prioritizing the group over the individual while individualist education focuses solely on the success of the individual. Considering your own experiences and backgrounds, discuss your values as they relate to collectivist and individualistic aspects of your own education. It is important to create a set of agreements to guide our community in and out of class. Which of the following do you think are most important to keep in mind for our time together in this course and why? What might you add to our guiding principles? W.A.I.T. (Why Am I Talking/Why Aren’t I Talking) Be curious Extend and receive grace Understand impact vs. intention Breathe and lean into discomfort Embrace diversity of experience What strategies have you found work well for you to succeed in learning both in and out of class that you want to continue this year? What are some things that have contributed to positive learning experiences in your courses that you would like to have in place for this course? What has contributed to negative experiences that you would like to prevent? Explorations Each of the data contexts below prompts a broad research goal. For each, sharpen the focus of that goal by coming up with more targeted research questions that: Can be studied with a regression exploration Can be studied with a classification exploration Can be studied with an unsupervised learning exploration Also, consider the impact of these research questions on society: Who or what might benefit from the collection of data, its analysis, or a prediction model based on it? Who or what might be harmed from the collection of data, its analysis, or a prediction model based on it? Context 1: The New York Times is trying to better understand the popularity of its different articles with the hope of using this understanding to improve hiring of writers and improve their website layout. Context 2: The Minnesota Department of Health is trying to better understand the different health trajectories of people who have contracted a certain illness to improve funding for relevant health services. Context 3: The campaign management team for a political candidate is trying to better understand voter turnout in different regions of the country to run a better campaign in the next election. "],["evaluating-regression-models.html", "Topic 2 Evaluating Regression Models Learning Goals Exercises", " Topic 2 Evaluating Regression Models Learning Goals Create and interpret residuals vs. fitted, residuals vs. predictor plots to identify improvements in modeling and address ethical concerns Interpret MSE, RMSE, MAE, and R-squared in a contextually meaningful way We’ll also start to develop some new ideas relating to our next topics. Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) (If you have not already installed the tidymodels package, install it with install.packages(\"tidymodels\").) library(readr) library(ggplot2) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidymodels) ## Registered S3 method overwritten by &#39;tune&#39;: ## method from ## required_pkgs.model_spec parsnip ## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ── ## ✔ broom 0.7.9 ✔ rsample 0.1.0 ## ✔ dials 0.0.10 ✔ tibble 3.1.3 ## ✔ infer 1.0.0 ✔ tidyr 1.1.3 ## ✔ modeldata 0.1.1 ✔ tune 0.1.6 ## ✔ parsnip 0.1.7 ✔ workflows 0.2.3 ## ✔ purrr 0.3.4 ✔ workflowsets 0.1.0 ## ✔ recipes 0.1.17 ✔ yardstick 0.0.8 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ yardstick::spec() masks readr::spec() ## ✖ recipes::step() masks stats::step() ## • Use tidymodels_prefer() to resolve common conflicts. tidymodels_prefer() bodyfat &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) ## Rows: 80 Columns: 17 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (17): fatBrozek, fatSiri, density, age, weight, height, neck, chest, abd... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Remove the fatBrozek and density variables bodyfat &lt;- bodyfat %&gt;% select(-fatBrozek, -density) Class investigations We’ll work through this section together to review concepts and code. You’ll then work on the remainder of the exercises in your groups. # Exploratory plots # Univariate distribution of outcome ggplot(bodyfat, aes(???)) + geom_???() # Scatterplot of fatSiri vs. weight ggplot(bodyfat, aes(???)) + geom_???() Let’s fit a linear regression model using tidymodels to predict body fat percentage from weight. lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) mod1 &lt;- fit(lm_spec, ?? ~ ??, data = bodyfat) mod1 %&gt;% tidy() We can use the predict() function to use our fit model to predict the outcome based on values in our original data. We can calculate residuals by taking our true outcome values and subtracting the predicted value, stored as .pred. mod1_output &lt;- mod1 %&gt;% predict(new_data = bodyfat) %&gt;% bind_cols(??) %&gt;% mutate(resid = ?? - ??) head(mod1_output) We can use this data frame to compute error metrics by hand or by using functions from the yardstick package. # MSE - what is the interpretation with units? mod1_output %&gt;% summarize(mean(resid^2)) # RMSE - what is the interpretation with units? mod1_output %&gt;% summarize(sqrt(mean(resid^2))) mod1_output %&gt;% rmse(truth = ??, estimate = .pred) # MAE - what is the interpretation with units? mod1_output %&gt;% summarize(mean(abs(resid))) mod1_output %&gt;% mae(truth = ??, estimate = .pred) # R-squared - interpretation? (unit-less) mod1_output %&gt;% summarize(1 - (var(resid) / var(fatSiri))) mod1_output %&gt;% rsq(truth = ??, estimate = .pred) …and to create residual plots: # Residuals vs. predictions ggplot(mod1_output, aes(x = .pred, y = resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() # Residuals vs. predictors (x&#39;s) ggplot(mod1_output, aes(x = height, y = resid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_classic() Exercise 1 First decide on what you think would be a good model by picking variables based on context. Fit this model, calling it mod_initial. (Remember that you can include several predictors with a + in the model formula - like y ~ x1+x2.) # Code to fit initial model Use residual plot explorations to check if you need to update your model. # Residual plot explorations Fit your updated model, and call it model_updated. # Code to fit updated model Exercise 2 Compute and contextually interpret relevant evaluation metrics for your model. # Code to compute evaluation metrics Exercise 3 Now that you’ve selected your best model, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the new_data to use bodyfat_test instead of bodyfat. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) Compare your evaluation metrics from Exercise 2 the metrics here. What do you notice? (Note: this observation is just based on your one particular fitted model. You’ll make some more comprehensive observations in the next exercise.) In general, do you think that models with more or fewer variables would perform better in this comparison? Explain. Exercise 4 The code below systematically looks at the same comparison that you made in Exercise 3 but for every possible linear regression model formed from inclusion/exclusion of the predictors (without transformations or interactions). See the plot below for the results. Note: You can try to run the code to make a plot of the results of this systematic investigation (it might take awhile). Feel free to inspect the code if you’re curious, but otherwise, don’t worry about understanding it fully. What do you notice? What do you wonder? get_maes &lt;- function(mod, train_data, test_data) { mod_output_train &lt;- bind_cols(train_data, predict(mod, new_data = train_data)) mod_output_test &lt;- bind_cols(test_data, predict(mod, new_data = test_data)) train_mae &lt;- mod_output_train %&gt;% mae(truth = fatSiri, estimate = .pred) %&gt;% pull(.estimate) test_mae &lt;- mod_output_test %&gt;% mae(truth = fatSiri, estimate = .pred) %&gt;% pull(.estimate) c(train_mae, test_mae) } possible_predictors &lt;- setdiff(colnames(bodyfat), c(&quot;fatSiri&quot;, &quot;hipin&quot;)) # Run time is while results &lt;- bind_rows(lapply(1:13, function(i) { combos &lt;- combn(possible_predictors, i) bind_rows(lapply(seq_len(ncol(combos)), function(j) { form &lt;- paste(&quot;fatSiri ~&quot;, paste(combos[,j], collapse = &quot;+&quot;)) mod &lt;- fit(lm_spec, as.formula(form), data = bodyfat) maes &lt;- get_maes(mod = mod, train_data = bodyfat, test_data = bodyfat_test) tibble( form = form, train_mae = maes[1], test_mae = maes[2], num_predictors = i ) })) })) save(results, file = &#39;allsubsets.RData&#39;) #I save the results so that I don&#39;t have to rerun the code above load(&#39;allsubsets.RData&#39;) # Relabel the num_predictors variable results &lt;- results %&gt;% mutate(num_predictors = factor(paste(&quot;# predictors:&quot;, num_predictors), levels = paste(&quot;# predictors:&quot;, 1:13))) # Plot results results %&gt;% ggplot(aes(x = train_mae, y = test_mae, color = num_predictors)) + geom_point() + coord_cartesian(xlim = c(0,7.5), ylim = c(0,7.5)) + geom_abline(slope = 1, intercept = 0) + facet_wrap(~ num_predictors) + guides(color = &quot;none&quot;) + labs(x = &quot;MAE on original data&quot;, y = &quot;MAE on new data on 172 men&quot;) + theme_classic() "],["overfitting.html", "Topic 3 Overfitting Learning Goals The tidymodels package Exercises", " Topic 3 Overfitting Learning Goals Explain why training/in-sample model evaluation metrics can provide a misleading view of true test/out-of-sample performance Implement testing and training sets in R using the tidymodels package Slides from today are available here. The tidymodels package Over this course, we will looking at a broad but linked set of specialized tools applied in statistical machine learning. Specialized tools generally require specialized code. Each tool has been developed separately and coded in a unique way. In order to facilitate and streamline the user experience, there have been attempts at creating a uniform interface, such as the caret R package. The developers of the caret package are no longer maintaining those packages. They are working on a newer package, called tidymodels. In this class, we will use the tidymodels package, which uses the tidyverse syntax you learned in Stat 155. The tidymodels package is a relatively new package and continues to be developed as we speak. This means that I’m learning with you and in a month or two, there may be improved functionality. As I introduced in the R code videos, we have a general workflow structure that includes a model specification and a recipe (formula + preprocessing steps). # Load the package library(tidymodels) tidymodels_prefer() # Set the seed for the random number generator set.seed(123) # Specify Model model_spec &lt;- linear_reg() %&gt;% # type of model set_engine(engine = __) %&gt;% # algorithm to fit the model set_args(__) %&gt;% # hyperparameters/tuning parameters are needed for some models set_mode(__) # regression or classification # Specify Recipe (if you have preprocessing steps) rec &lt;- recipe(formula, data) %&gt;% step_{FILLIN}() %&gt;% step_{FILLIN}() # Create Workflow (Model + Recipe) model_wf &lt;- workflow() %&gt;% add_recipe(rec) %&gt;% #or add_formula() add_model(model_spec) We can fit that workflow to training data. # Fit Model to training data (without a recipe) fit_model &lt;- fit(model_spec, formula, data_train) # Fit Model &amp; Recipe to training data fit_model &lt;- fit(model_wf, data_train) And then we can evaluate that fit model on testing data (new data that has not been used to fit the model). # Evaluate on testing data model_output &lt;- fit_model %&gt;% predict(new_data = data_test) %&gt;% # this function will apply recipe to new_data and do prediction bind_cols(data_test) reg_metrics &lt;- metric_set(rmse, rsq, mae) model_output %&gt;% reg_metrics(truth = __, estimate = .pred) The power of tidymodels is that it allows us to streamline the vast world of machine learning techniques into one common syntax. On top of \"lm\", there are many other different machine learning methods that we can use. In the exercises below, you’ll need to adapt the code above to fit a linear regression model (engine = \"lm\"). Exercises You can download a template RMarkdown file to start from here. Context We’ll be working with a dataset containing physical measurements on 80 adult males. These measurements include body fat percentage estimates as well as body circumference measurements. fatBrozek: Percent body fat using Brozek’s equation: 457/Density - 414.2 fatSiri: Percent body fat using Siri’s equation: 495/Density - 450 density: Density determined from underwater weighing (gm/cm^3). age: Age (years) weight: Weight (lbs) height: Height (inches) neck: Neck circumference (cm) chest: Chest circumference (cm) abdomen: Abdomen circumference (cm) hip: Hip circumference (cm) thigh: Thigh circumference (cm) knee: Knee circumference (cm) ankle: Ankle circumference (cm) biceps: Biceps (extended) circumference (cm) forearm: Forearm circumference (cm) wrist: Wrist circumference (cm) It takes a lot of effort to estimate body fat percentage accurately through underwater weighing. The goal is to build the best predictive model for fatSiri using just circumference measurements, which are more easily attainable. (We won’t use fatBrozek or density as predictors because they’re other outcome variables.) library(readr) library(ggplot2) library(dplyr) library(tidymodels) tidymodels_prefer() bodyfat_train &lt;- read_csv(&quot;https://www.dropbox.com/s/js2gxnazybokbzh/bodyfat_train.csv?dl=1&quot;) ## Rows: 80 Columns: 17 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (17): fatBrozek, fatSiri, density, age, weight, height, neck, chest, abd... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Remove the fatBrozek and density variables (and one variable that a replicate of others) bodyfat_train &lt;- bodyfat_train %&gt;% select(-fatBrozek, -density, -hipin) Exercise 1: 5 models Consider the 5 models below: lm_spec &lt;- linear_reg() %&gt;% set_engine(engine = &#39;lm&#39;) %&gt;% set_mode(&#39;regression&#39;) mod1 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm, data = bodyfat_train) mod2 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps, data = bodyfat_train) mod3 &lt;- fit(lm_spec, fatSiri ~ age+weight+neck+abdomen+thigh+forearm+biceps+chest+hip, data = bodyfat_train) mod4 &lt;- fit(lm_spec, fatSiri ~ ., # The . means all predictors data = bodyfat_train) bf_recipe &lt;- recipe(fatSiri ~ ., data = bodyfat_train) %&gt;% step_normalize(all_numeric_predictors()) bf_wf &lt;- workflow() %&gt;% add_recipe(bf_recipe) %&gt;% add_model(lm_spec) mod5 &lt;- fit(bf_wf, data = bodyfat_train) STAT 155 review: Look at the tidy() of mod1. Contextually interpret the coefficient for the weight predictor. Is anything surprising? Why might this be? Explain how mod5 is different than mod4. You may want to look at bf_recipe %&gt;% prep(bodyfat_train) %&gt;% juice() to see the preprocessed training data. Which model will have the lowest training RMSE, and why? Explain before calculating (that is part d). Compute the training RMSE for models 1 through 5 to check your answer for part c. Write a sentence interpreting one of values of RMSE in context. Which model do you think is the “best”? You may calculate MAE and R squared as well to justify your answer. Which model do you think will perform worst on new test data? Why? Exercise 2: Visualizing Predictions Sequentially run the code below, ending before pipe, comma, or +. For each row of code below, discuss what it does. Add comments to the end of the line after the pipe (with # in front) to explain what each line does. mod5 %&gt;% #comment here tidy() %&gt;% # slice(-1) %&gt;% # mutate(lower = estimate - 1.96*std.error, upper = estimate + 1.96*std.error) %&gt;% # ggplot() + # geom_vline(xintercept=0, linetype=4) + # geom_point(aes(x=estimate, y=term)) + # geom_segment(aes(y=term, yend=term, x=lower, xend=upper), arrow = arrow(angle=90, ends=&#39;both&#39;, length = unit(0.1, &#39;cm&#39;))) + # labs(x = &#39;Coefficient estimate (95% CI)&#39;, y = &#39;Feature&#39;) + # theme_classic() # Sequentially run each line of code (below), end before pipe or comma. For each row of code below, discuss what it does. Add comments to the end of the line after the pipe (with # in front) to explain what each line does. bodyfat_train %&gt;% # mutate(id = row_number()) %&gt;% # pivot_longer(-c(fatSiri, id), names_to = &#39;key&#39;, values_to = &#39;value&#39;) %&gt;% # right_join( # (mod4 %&gt;% tidy() %&gt;% slice(-1) %&gt;% select(term, estimate)), # by = c(&#39;key&#39;=&#39;term&#39;) ) %&gt;% #right_join finishes here mutate(effect = value * estimate) %&gt;% # ggplot(aes(x = effect, y = key)) + # geom_boxplot() + # geom_vline(xintercept = 0, color = &#39;grey&#39;) + # labs(x = &#39;Effect/Contribution to Predicted BodyFat Percent&#39;, y = &#39;Feature&#39;) + # theme_classic() # Exercise 3: Evaluating on Test Data Now that you’ve thought about how well the models might perform on test data, deploy it in the real world by applying it to a new set of 172 adult males. You’ll need to update the new_data to use bodyfat_test instead of bodyfat_train. bodyfat_test &lt;- read_csv(&quot;https://www.dropbox.com/s/7gizws208u0oywq/bodyfat_test.csv?dl=1&quot;) # Use fit/trained models and evaluate on test data Calculate the test RMSE, MAE, and R squared for all five of the models. Look back to Exercise 1 and see which model you thought was “best” based on the training data. Is that the “best” model in terms of predicting on new data? Explain. In “real life” we only have one data set. To get a sense of predictive performance on new test data, we could split our data into two groups. Discuss pros and cons of ways you might split the data. How big should the training set be? How big should the testing set be? Exercise 4: Overfitting If you have time, consider the following relationship. Imagine a set of predictions that is overfit to this training data. You are not limited to lines. Draw a picture of that function of predictions on a piece of paper. set.seed(123) data_train &lt;- tibble::tibble( x = runif(15,0,7), y = x^2 + rnorm(15,sd = 7) ) data_train %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + theme_classic() "],["homework-1.html", "Homework 1 Project Work Portfolio Work Reflection", " Homework 1 Due Friday, February 4th at 9:00 CST on Moodle Please turn in a single PDF document containing (1) your responses for the Project Work and Reflection sections and (2) a LINK to the Google Doc with your responses for the Portfolio Work section. Project Work Goal: Find a dataset (or datasets) to use for your final project, and start to get to know the data. Details: Your dataset(s) should allow you to perform a (1) regression, (2) classification, and (3) unsupervised learning analysis. The following resources are good places to start looking for data: Google Dataset Search Kaggle UCI Machine Learning Repository Harvard Dataverse You’ll end up working in a group of 2-4 people on the project, but please complete this initial work individually. It’s fine if you and a potential/future group member end up using the same dataset for this homework and collaborate on the finding of data, but complete the short bit of writing (below) individually. Check in with the instructor early if you need help. Deliverables: Write 1-2 paragraphs (no more than 350 words) summarizing: The information in the dataset(s) and the context behind the data. Use the prompts below to guide your thoughts. (Note: in some situations, there may be incomplete information on the data context. That’s fine. Just do your best to summarize what information is available, and acknowledge the lack of information where relevant.) What are the cases? Broadly describe the variables contained in the data. Who collected the data? When, why, and how? 3 research questions 1 that can be investigated in a regression setting 1 that can be investigated in a classification setting 1 that can be investigated in an unsupervised learning setting Also make sure that you can read the data into R. You don’t need to do any analysis in R yet, but making sure that you can read the data will make the next steps go more smoothly. Portfolio Work Page maximum: 2 pages of text (pictures don’t count) Organization: Your choice! Use titles and section headings that make sense to you. (It probably makes sense to have a separate section for each method.) Deliverables: Put your responses for this part in a Google Doc, and update the link sharing so that anyone with the link at Macalester College can edit. Include the URL for the Google Doc in your submission. Note: Some prompts below may seem very open-ended. This is intentional. Crafting good responses requires looking back through our material to organize the concepts in a coherent, thematic way, which is extremely useful for your learning. Concepts to address: Evaluating regression models: Describe how residuals are central to the evaluation of regression models. Explain how they arise in quantitative evaluation metrics and how they are used in evaluation plots. Include examples of plots that show desirable and undesirable model behavior (feel free to draw them by hand if you wish) and what steps can be taken to address that undesirable behavior. Overfitting: The concept video used the analogy of a cat picture model to explain overfitting. Come up with your own analogy to explain overfitting. Cross-validation: In your own words, explain the rationale for cross-validation in relation to overfitting and model evaluation. Describe the algorithm in your own words in at most 2 sentences. Reflection Ethics: Read the article Amazon scraps secret AI recruiting tool that showed bias against women or watch the rest of Coded Bias (available on Netflix). Write a short (roughly 250 words), thoughtful response about the themes and cautions that the article or movie brings forth. Reflection: Write a short, thoughtful reflection about how things are going in the course. Feel free to use whichever prompts below resonate most with you, but don’t feel limited to these prompts. How is your understanding of the material? What ideas/topics have stuck out for you? How is group work going? Any strategies for improving collaboration that you want to try out next week? How is your work/life balance going? Any new activities or strategies that you want to try out for next week? Self-Assessment: Before turning in this assignment on Moodle, go to the individual rubric shared with you and complete the self-assessment for the general skills. After “HW1:”, assess yourself on each of the general skills. Assessing yourself is hard. We must practice this skill. These “grades” you give yourself are intended to have you stop and think about your learning as you grow and develop the general skills and deepen your understanding of the course topics. These grades do not map directly to a final grade. "],["final-project.html", "Final Project Requirements Expectations", " Final Project Requirements You will be analyzing a dataset using a regression analysis, a classification analysis and an unsupervised learning analysis. Collaboration: You will work in teams of 2-4 members. The weekly homework assignments will note whether work for that week should be submitted individually or if just one team member should submit work. There will be a required synthesis of the weekly homework investigations at the end of the course that will be done in your group. Final deliverables: Only one team member has to submit these materials to Moodle. Deadline for final files: TBD Submit a final knitted HTML file of a CODE APPENDIX (must knit without errors) that is CLEARLY organized and easy to navigate plus the corresponding Rmd file used to create the html file. To ensure code is run and output is provided, make sure that you have the following code in the first R chunk at the top of the Rmd file. knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE) A 10-15 minute video presentation of your project that addresses the items in the Expectations below. (Recording the presentation over Zoom is a good option for creating the video. You can record to your computer or to the cloud.) Upload the video to Google Drive &amp; share a link in Moodle (with Mac Viewer sharing) All team members should have an equal speaking role in the presentation. In order to record your presentation, Start a Zoom meeting and invite your project mates. One of your share your screen with presentation slides (recommended: Google Slides or Powerpoint). Please have everyone turn your video on so that we can see who is speaking. When you are ready to start, the host of the meeting (who ever started the meeting) can click Record on this Computer. I highly recommend that someone else start a timer so that you can make sure you keep the presentation to 15 minutes max. Start presenting! You can Pause the recording, as needed, and then press start recording again. When you have finished recording, you can press Stop Recording. When you end the meeting, the recording (an mp4 file) will be downloaded to the computer of the individual who pressed Record. Upload the video to Google Drive &amp; share a link in Moodle Expectations In your final presentation, you should address the following things: Data context Clearly describe what the cases in the final clean dataset represent. Broadly describe the variables used in your analyses. Who collected the data? When, why, and how? Answer as much of this as the available information allows. Research questions Research question(s)/motivation for the regression task; make clear the outcome variable and its units. Research question(s)/motivation for the classification task; make clear the outcome variable and its possible categories. Research question(s)/motivation for the unsupervised learning task. Regression: Methods Describe the models used. Describe what you did to evaluate models. Indicate how you estimated quantitative evaluation metrics. Indicate what plots you used to evaluate models. Describe the goals / purpose of the methods used in the overall context of your research investigations. Regression: Results Summarize your final model and justify your model choice (see below for ways to justify your choice). Compare the different models in light of evaluation metrics, plots, variable importance, and data context. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated CV metric as well as its standard deviation. Broadly summarize conclusions from looking at these CV evaluation metrics and their measures of uncertainty. Summarize conclusions from residual plots from initial models (don’t have to display them though). Show and interpret some representative examples of residual plots for your final model. Does the model show acceptable results in terms of any systematic biases? Regression: Conclusions Interpret you final model (show plots of estimated non-linear functions, or slope coefficients) for important predictors, and provide some general interpretations of what you learn from these Interpret evaluation metric(s) for the final model in context with units. Does the model show an acceptable amount of error? Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. Classification: Methods Indicate at least 2 different methods used to answer your classification research question. Describe what you did to evaluate the models explored. Indicate how you estimated quantitative evaluation metrics. Describe the goals / purpose of the methods used in the overall context of your research investigations. Classification: Results Summarize your final model and justify your model choice (see below for ways to justify your choice). Compare the different classification models tried in light of evaluation metrics, variable importance, and data context. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.) Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty. Classification: Conclusions Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results. Unsupervised Learning: Clustering Choose one method for clustering Justify the choice of features included in a distance measure based on the research goals Justify the choice of \\(k\\) and summarize resulting clusters Interpret the clusters qualitatively Evaluate clusters quantitatively (kmeans: within cluster sum of squares, pam: silhouette, hclust: height of cut on dendrogram) If appropriate, show visuals to justify your choices. Summarize what information you gain from the clustering in context (tell a story) Code Knitted, error-free HTML and corresponding Rmd file submitted Code corresponding to all analyses above is present and correct "]]
